{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-04-05T06:45:24.202037Z",
     "iopub.status.busy": "2022-04-05T06:45:24.201384Z",
     "iopub.status.idle": "2022-04-05T06:45:25.031387Z",
     "shell.execute_reply": "2022-04-05T06:45:25.030651Z",
     "shell.execute_reply.started": "2022-04-03T09:44:23.727746Z"
    },
    "papermill": {
     "duration": 0.857016,
     "end_time": "2022-04-05T06:45:25.031603",
     "exception": false,
     "start_time": "2022-04-05T06:45:24.174587",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "tf.random.set_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.__version__)\n",
    "print(\"GPU disponible:\", tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature lists\n",
    "standard_features = [\"DMP_5\", \"DMP_10\", \"DMN_5\", \"DMN_10\", \"MACD_8_21_5\", \"AO_5_10\",\n",
    "    \"EMA_15\", \"MA_20\", \"KAMA_3_2_10\", \"CO\", \"C2O2\", \"C3O3\",\n",
    "    \"net_add_ask_size\", \"net_add_bid_size\", \"Bollinger_Upper\", \"Bollinger_Lower\"\n",
    "]\n",
    "\n",
    "minmax_features = [\n",
    "    \"ADX_10\", \"ADX_7\", \"ADX_5\", \"STOCHk_7_3_3\", \"STOCHd_7_3_3\", \"RSI_7\", \"time_since_open\"\n",
    "]\n",
    "\n",
    "unscaled_features = ['market_session']\n",
    "features = standard_features + minmax_features + unscaled_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_rate = \"1s\"\n",
    "prediction_column = \"Target_close\"\n",
    "batch_size = 16\n",
    "epochs = 10\n",
    "look_back = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_preprocessing import process_and_combine_data\n",
    "import os\n",
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "start_date = \"2024-10-02\"\n",
    "end_date = \"2024-10-04\"\n",
    "\n",
    "stock_name = os.getenv(\"STOCK_NAME\")\n",
    "# used AAL instead\n",
    "all_data = process_and_combine_data(start_date, end_date, data_folder=\"/home/janis/3A/EA/HFT_QR_RL/data/smash4/DB_MBP_10/\" + stock_name, sampling_rate=sampling_rate)\n",
    "\n",
    "print(all_data.columns)\n",
    "\n",
    "all_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.Target_close.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(all_data.index, all_data[\"mid_price_close\"], label=\"Mid Price Mean\", color=\"blue\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Mid Price Close\")\n",
    "plt.title(\"Mid Price Close Over Time\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Splitting\n",
    "train_size = int(len(all_data) * 0.7)\n",
    "val_size = int(len(all_data) * 0.1)\n",
    "test_size = len(all_data) - train_size - val_size\n",
    "\n",
    "train_df = all_data.iloc[:train_size, :]\n",
    "val_df = all_data.iloc[train_size:train_size + val_size, :]\n",
    "test_df = all_data.iloc[train_size + val_size:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import Sequence\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from collections import Counter\n",
    "\n",
    "standard_indices = [features.index(f) for f in standard_features]\n",
    "minmax_indices = [features.index(f) for f in minmax_features]\n",
    "unscaled_indices = [features.index(f) for f in unscaled_features]\n",
    "\n",
    "class TimeSeriesScalerGenerator(Sequence):\n",
    "    def __init__(self, data, target, look_back, batch_size=32, oversample=False, **kwargs):\n",
    "        \"\"\"\n",
    "        Custom Timeseries Generator with per-sequence scaling and optional oversampling.\n",
    "        \n",
    "        Args:\n",
    "            data (pd.DataFrame): DataFrame with feature columns.\n",
    "            target (str): Target column name.\n",
    "            look_back (int): Number of past time steps per sample.\n",
    "            batch_size (int): Batch size.\n",
    "            oversample (bool): Whether to oversample minority classes.\n",
    "        \"\"\"\n",
    "        super().__init__(**kwargs)  # Call the parent class constructor with kwargs\n",
    "\n",
    "        self.data = data[features].values  # Extract feature matrix\n",
    "        self.targets = data[target].values.astype(int)  # Extract target labels\n",
    "        self.look_back = look_back\n",
    "        self.batch_size = batch_size\n",
    "        self.indices = np.arange(len(data) - look_back)\n",
    "        self.true_labels = self._extract_true_labels()\n",
    "\n",
    "        # Oversample minority classes if enabled\n",
    "        if oversample:\n",
    "            self._oversample_minority_classes()\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Number of batches per epoch.\"\"\"\n",
    "        return int(np.ceil(len(self.indices) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Generates one batch of data.\"\"\"\n",
    "        batch_indices = self.indices[idx * self.batch_size : (idx + 1) * self.batch_size]\n",
    "\n",
    "        # Extract sequences efficiently using list slicing\n",
    "        batch_data = np.array([self.data[i : i + self.look_back] for i in batch_indices])\n",
    "\n",
    "        # Preallocate arrays for batch\n",
    "        X_batch = np.empty((len(batch_indices), self.look_back, len(features)), dtype=np.float32)\n",
    "        y_batch = np.empty(len(batch_indices), dtype=np.int32)\n",
    "\n",
    "        # Scale each sequence individually\n",
    "        for i, seq in enumerate(batch_data):\n",
    "            standard_scaler = StandardScaler()\n",
    "            minmax_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "            seq_standard = standard_scaler.fit_transform(seq[:, standard_indices])\n",
    "            seq_minmax = minmax_scaler.fit_transform(seq[:, minmax_indices])\n",
    "            seq_unscaled = (\n",
    "                seq[:, unscaled_indices] \n",
    "                if unscaled_features else np.empty((self.look_back, 0))\n",
    "            )\n",
    "\n",
    "            X_batch[i] = np.hstack((seq_standard, seq_minmax, seq_unscaled))\n",
    "            y_batch[i] = self.targets[batch_indices[i] + self.look_back]\n",
    "\n",
    "        return X_batch, y_batch\n",
    "    \n",
    "    def _extract_true_labels(self):\n",
    "        \"\"\"Extract all true labels for the entire dataset.\"\"\"\n",
    "        return np.array([self.targets[i + self.look_back] for i in self.indices])\n",
    "\n",
    "    def _oversample_minority_classes(self):\n",
    "        \"\"\"Oversample minority classes by duplicating their sequences.\"\"\"\n",
    "        # Count class distribution\n",
    "        class_counts = Counter(self.true_labels)\n",
    "        max_count = max(class_counts.values())\n",
    "\n",
    "        # Collect indices for each class\n",
    "        class_indices = {label: np.where(self.true_labels == label)[0] for label in class_counts}\n",
    "\n",
    "        # Oversample minority classes\n",
    "        oversampled_indices = []\n",
    "        for label, indices in class_indices.items():\n",
    "            repeat_count = max_count // len(indices)  # Number of times to repeat the minority class\n",
    "            oversampled_indices.extend(np.repeat(indices, repeat_count))\n",
    "            oversampled_indices.extend(np.random.choice(indices, max_count % len(indices), replace=True))\n",
    "\n",
    "        # Update indices and true_labels\n",
    "        self.indices = np.array(oversampled_indices)\n",
    "        self.true_labels = self.true_labels[self.indices]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = TimeSeriesScalerGenerator(train_df, prediction_column, look_back, batch_size, oversample=True)\n",
    "val_gen = TimeSeriesScalerGenerator(val_df, prediction_column, look_back, batch_size)\n",
    "test_gen = TimeSeriesScalerGenerator(test_df, prediction_column, look_back, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from keras import layers\n",
    "\n",
    "# early_stop = EarlyStopping(\n",
    "#     monitor=\"val_loss\",  # Monitor validation loss\n",
    "#     patience=3,          # Stop after 3 epochs of no improvement\n",
    "#     restore_best_weights=True  # Restore best weights after stopping\n",
    "# )\n",
    "\n",
    "\n",
    "input_size = len(features)\n",
    "\n",
    "inputs = layers.Input(shape=(look_back, input_size))\n",
    "\n",
    "# First LSTM layer\n",
    "x = layers.LSTM(256, return_sequences=True)(inputs)\n",
    "x = layers.BatchNormalization()(x)\n",
    "\n",
    "# MultiHeadAttention requires query, key, value (use same for self-attention)\n",
    "attn_output = layers.MultiHeadAttention(num_heads=4, key_dim=64)(x, x, x)\n",
    "x = layers.Add()([x, attn_output])  # Residual connection (optional)\n",
    "x = layers.LayerNormalization()(x)\n",
    "x = layers.Dropout(0.3)(x)  # Dropout after attention\n",
    "\n",
    "# Second LSTM layer\n",
    "x = layers.LSTM(128, return_sequences=False)(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "\n",
    "# Dense layers\n",
    "x = layers.Dense(64, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "outputs = layers.Dense(3, activation=\"softmax\")(x)\n",
    "\n",
    "# Create model\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.001)\n",
    "# optimizer = keras.optimizers.SGD(learning_rate=0.01)\n",
    "\n",
    "model.compile(optimizer=optimizer, \n",
    "              loss=\"sparse_categorical_crossentropy\", \n",
    "              metrics=[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train Model\n",
    "model.fit(train_gen, validation_data=val_gen, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spécifier explicitement l'utilisation du GPU\n",
    "with tf.device('/GPU:0'):\n",
    "    model.fit(\n",
    "        train_gen,\n",
    "        validation_data=val_gen,\n",
    "        epochs=epochs,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "\n",
    "def plot_evaluation_metrics(y_true, y_pred, log_probabilities):\n",
    "    probabilities = np.exp(log_probabilities)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "    # Confusion Matrix (updated for 3 classes)\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    sns.heatmap(\n",
    "        cm,\n",
    "        annot=True,\n",
    "        fmt=\"d\",\n",
    "        cmap=\"Blues\",\n",
    "        ax=axes[0],\n",
    "        xticklabels=[\"Down (0)\", \"Constant (1)\", \"Up (2)\"],  # Updated class labels with meaning\n",
    "        yticklabels=[\"Down (0)\", \"Constant (1)\", \"Up (2)\"],  # Updated class labels with meaning\n",
    "    )\n",
    "    axes[0].set_title(\"Confusion Matrix\")\n",
    "    axes[0].set_xlabel(\"Predicted\")\n",
    "    axes[0].set_ylabel(\"True\")\n",
    "\n",
    "    # Histogram of Predicted Probabilities (updated for 3 classes)\n",
    "    for i, class_label in enumerate([\"Down (0)\", \"Constant (1)\", \"Up (2)\"]):  # Updated class labels with meaning\n",
    "        sns.histplot(\n",
    "            probabilities[y_true == i][:, i], bins=30, label=class_label, ax=axes[1]\n",
    "        )\n",
    "    axes[1].set_title(\"Probability Distribution\")\n",
    "    axes[1].set_xlabel(\"Predicted Probability\")\n",
    "    axes[1].legend()\n",
    "\n",
    "    # Scatter Plot of Predictions (updated for 3 classes)\n",
    "    scatter = sns.scatterplot(\n",
    "        x=np.arange(len(probabilities)),\n",
    "        y=probabilities.max(axis=1),\n",
    "        hue=y_true,\n",
    "        palette={0: \"red\", 1: \"blue\", 2: \"green\"},  # Updated palette for 0, 1, 2\n",
    "        alpha=0.7,\n",
    "        ax=axes[2],\n",
    "    )\n",
    "    axes[2].set_title(\"Scatter Plot of Predictions\")\n",
    "    axes[2].set_xlabel(\"Sample Index\")\n",
    "    axes[2].set_ylabel(\"Max Predicted Probability\")\n",
    "\n",
    "    handles, labels = scatter.get_legend_handles_labels()\n",
    "    new_labels = [\"Down (0)\", \"Constant (1)\", \"Up (2)\"]  # Updated class labels with meaning\n",
    "    axes[2].legend(handles, new_labels, title=\"True Class\")\n",
    "\n",
    "    axes[2].tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Print Evaluation Metrics\n",
    "    print(f\"Accuracy: {accuracy_score(y_true, y_pred):.4f}\")\n",
    "    print(classification_report(y_true, y_pred, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_prob = model.predict(test_gen)\n",
    "y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "log_probabilities = np.log(y_pred_prob)\n",
    "\n",
    "y_test = test_gen.true_labels\n",
    "\n",
    "# Assuming y_test is the true labels for the test set\n",
    "plot_evaluation_metrics(y_test, y_pred, log_probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement a random strategy\n",
    "random_y_pred = np.random.randint(0, 3, size=len(y_test))\n",
    "random_log_probabilities = np.log(np.random.rand(len(y_test), 3))\n",
    "\n",
    "# Plot evaluation metrics for the random strategy\n",
    "plot_evaluation_metrics(y_test, random_y_pred, random_log_probabilities)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 233.420481,
   "end_time": "2022-04-05T06:49:11.920913",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-04-05T06:45:18.500432",
   "version": "2.2.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
